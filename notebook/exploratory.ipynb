{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f96e85f",
   "metadata": {},
   "source": [
    "# FMA Music Genre Clustering - Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the **Free Music Archive (FMA)** dataset for music genre classification using Variational Autoencoders (VAEs).\n",
    "\n",
    "**Dataset:** FMA - 8 genres (blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock)\n",
    "\n",
    "**Features:**\n",
    "- MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "- Mel Spectrograms\n",
    "\n",
    "**Models:**\n",
    "- Basic VAE (Easy Task)\n",
    "- Convolutional VAE (Medium Task)  \n",
    "- Œ≤-VAE with disentangled representations (Hard Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports for FMA dataset exploration\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(Path('.').resolve()))\n",
    "\n",
    "from src import config\n",
    "from src.features import load_audio, extract_mfcc, extract_mel_spectrogram\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"FMA Dataset Exploration - Setup complete!\")\n",
    "print(f\"Expected genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403e30f",
   "metadata": {},
   "source": [
    "## 1. Explore FMA Raw Audio Data\n",
    "\n",
    "The FMA dataset contains 30-second audio clips across multiple genres. Let's examine the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b558133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check FMA data directory structure\n",
    "data_dir = config.RAW_DATA_DIR\n",
    "print(f\"FMA Data directory: {data_dir}\")\n",
    "print(f\"Exists: {data_dir.exists()}\")\n",
    "\n",
    "# Expected FMA genres\n",
    "fma_genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "if data_dir.exists():\n",
    "    genres = [d.name for d in data_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"\\nGenres found: {genres}\")\n",
    "    \n",
    "    total_files = 0\n",
    "    for genre in genres:\n",
    "        genre_dir = data_dir / genre\n",
    "        audio_files = list(genre_dir.glob('*.wav')) + list(genre_dir.glob('*.mp3'))\n",
    "        total_files += len(audio_files)\n",
    "        print(f\"  {genre}: {len(audio_files)} files\")\n",
    "    print(f\"\\nTotal audio files: {total_files}\")\n",
    "else:\n",
    "    print(\"\\nPlease download FMA dataset to the data/raw directory!\")\n",
    "    print(\"Structure should be:\")\n",
    "    print(\"  data/raw/blues/*.wav\")\n",
    "    print(\"  data/raw/classical/*.wav\")\n",
    "    print(\"  data/raw/country/*.wav\")\n",
    "    print(\"  ... etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize a sample FMA audio file\n",
    "def analyze_audio_sample(audio_path):\n",
    "    \"\"\"Load and visualize a single FMA audio file.\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE, duration=config.AUDIO_DURATION)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    axes[0].set_title(f'FMA Waveform: {Path(audio_path).name}')\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0])\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.N_MELS)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(mel_spec_db, sr=sr, hop_length=config.HOP_LENGTH,\n",
    "                                    x_axis='time', y_axis='mel', ax=axes[1])\n",
    "    axes[1].set_title('Mel Spectrogram')\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    # MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config.N_MFCC)\n",
    "    img2 = librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=axes[2])\n",
    "    axes[2].set_title(f'MFCC ({config.N_MFCC} coefficients)')\n",
    "    fig.colorbar(img2, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y, sr\n",
    "\n",
    "# Find a sample FMA file\n",
    "if data_dir.exists():\n",
    "    sample_files = list(data_dir.rglob('*.wav'))[:1] + list(data_dir.rglob('*.mp3'))[:1]\n",
    "    if sample_files:\n",
    "        y, sr = analyze_audio_sample(sample_files[0])\n",
    "        print(f\"\\nFMA Audio properties:\")\n",
    "        print(f\"  Sample rate: {sr} Hz\")\n",
    "        print(f\"  Duration: {len(y)/sr:.2f} seconds\")\n",
    "        print(f\"  Samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play audio sample (Jupyter only)\n",
    "if 'y' in dir() and 'sr' in dir():\n",
    "    display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f349a",
   "metadata": {},
   "source": [
    "## 2. Compare FMA Features Across Genres\n",
    "\n",
    "Let's compare acoustic features (Mel Spectrograms and MFCCs) across different FMA genres to understand their distinguishing characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fma_genre_features(data_dir):\n",
    "    \"\"\"Compare MFCC features across different FMA genres.\"\"\"\n",
    "    genres = [d.name for d in data_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(genres), 2, figsize=(14, 4*len(genres)))\n",
    "    if len(genres) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, genre in enumerate(genres):\n",
    "        genre_dir = data_dir / genre\n",
    "        audio_files = list(genre_dir.glob('*.wav')) + list(genre_dir.glob('*.mp3'))\n",
    "        \n",
    "        if audio_files:\n",
    "            # Load first file from FMA genre\n",
    "            y, sr = librosa.load(audio_files[0], sr=config.SAMPLE_RATE, duration=config.AUDIO_DURATION)\n",
    "            \n",
    "            # Mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.N_MELS)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            librosa.display.specshow(mel_spec_db, sr=sr, ax=axes[i, 0], x_axis='time', y_axis='mel')\n",
    "            axes[i, 0].set_title(f'FMA {genre} - Mel Spectrogram')\n",
    "            \n",
    "            # MFCC\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config.N_MFCC)\n",
    "            librosa.display.specshow(mfccs, sr=sr, ax=axes[i, 1], x_axis='time')\n",
    "            axes[i, 1].set_title(f'FMA {genre} - MFCC')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if data_dir.exists():\n",
    "    compare_fma_genre_features(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcc890",
   "metadata": {},
   "source": [
    "## 3. Load Processed FMA Features\n",
    "\n",
    "Load pre-extracted MFCC and Mel-spectrogram features from the FMA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7525c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import load_processed_data\n",
    "\n",
    "# Try loading processed FMA features\n",
    "try:\n",
    "    mfcc_features, labels, genres = load_processed_data('mfcc')\n",
    "    print(f\"FMA MFCC features loaded: {mfcc_features.shape}\")\n",
    "    print(f\"Labels: {labels.shape}, Genres: {genres}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FMA MFCC features not found. Run preprocessing first!\")\n",
    "    print(\"  python -m src.preprocess_fma\")\n",
    "\n",
    "try:\n",
    "    mel_features, labels, genres = load_processed_data('mel_spectrogram')\n",
    "    print(f\"FMA Mel-Spectrogram features loaded: {mel_features.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FMA Mel-Spectrogram features not found. Run preprocessing first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FMA feature distributions\n",
    "if 'mfcc_features' in dir():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Feature value distribution\n",
    "    axes[0].hist(mfcc_features.flatten(), bins=50, alpha=0.7, color='steelblue')\n",
    "    axes[0].set_title('FMA MFCC Feature Value Distribution')\n",
    "    axes[0].set_xlabel('Value')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Class distribution across FMA genres\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique)))\n",
    "    axes[1].bar([genres[i] for i in unique], counts, color=colors)\n",
    "    axes[1].set_title('FMA Genre Distribution')\n",
    "    axes[1].set_xlabel('Genre')\n",
    "    axes[1].set_ylabel('Sample Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFMA Dataset Statistics:\")\n",
    "    for i, genre in enumerate(genres):\n",
    "        count = np.sum(labels == i)\n",
    "        print(f\"  {genre}: {count} samples ({100*count/len(labels):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb7a13",
   "metadata": {},
   "source": [
    "## 4. Load and Analyze VAE Results on FMA\n",
    "\n",
    "Analyze clustering results from the VAE models trained on the FMA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5887932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metrics from all FMA tasks\n",
    "results_dirs = {\n",
    "    'Easy (Basic VAE)': config.RESULTS_DIR / 'easy_task',\n",
    "    'Medium (Conv-VAE)': config.RESULTS_DIR / 'medium_task',\n",
    "    'Hard (Beta-VAE)': config.RESULTS_DIR / 'hard_task'\n",
    "}\n",
    "\n",
    "all_metrics = []\n",
    "for task_name, results_dir in results_dirs.items():\n",
    "    metrics_file = results_dir / 'metrics.csv' if 'Easy' in task_name else results_dir / 'all_metrics.csv'\n",
    "    if metrics_file.exists():\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        df['task'] = task_name\n",
    "        all_metrics.append(df)\n",
    "        print(f\"‚úì {task_name} FMA results loaded\")\n",
    "    else:\n",
    "        print(f\"‚úó {task_name} FMA results not found (run training first)\")\n",
    "\n",
    "if all_metrics:\n",
    "    combined_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FMA COMBINED RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "if 'combined_df' in dir() and len(combined_df) > 0:\n",
    "    metrics_to_plot = ['silhouette_score', 'calinski_harabasz_index', \n",
    "                       'adjusted_rand_index', 'normalized_mutual_info']\n",
    "    \n",
    "    available_metrics = [m for m in metrics_to_plot if m in combined_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, metric in enumerate(available_metrics[:4]):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Group by method\n",
    "            if 'method' in combined_df.columns:\n",
    "                pivot = combined_df.pivot_table(values=metric, index='method', \n",
    "                                                columns='task' if 'task' in combined_df.columns else None)\n",
    "                pivot.plot(kind='bar', ax=ax)\n",
    "            else:\n",
    "                combined_df.plot(y=metric, kind='bar', ax=ax)\n",
    "            \n",
    "            ax.set_title(metric.replace('_', ' ').title())\n",
    "            ax.set_xlabel('')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04ed20",
   "metadata": {},
   "source": [
    "## 5. Load and Visualize FMA Latent Space\n",
    "\n",
    "Visualize the latent space learned by VAE models on FMA genre data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import plot_tsne, plot_umap\n",
    "from src.vae import VAE\n",
    "from src.beta_vae import BetaVAE\n",
    "\n",
    "# Load best model and extract latent features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check for saved models\n",
    "easy_model_path = config.RESULTS_DIR / 'easy_task' / 'vae_best.pth'\n",
    "hard_model_path = config.RESULTS_DIR / 'hard_task' / 'beta_vae_best.pth'\n",
    "\n",
    "if easy_model_path.exists():\n",
    "    print(f\"‚úì Easy task model found\")\n",
    "if hard_model_path.exists():\n",
    "    print(f\"‚úì Hard task model found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20315563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive t-SNE visualization of FMA features\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "if 'mfcc_features' in dir():\n",
    "    print(\"Computing t-SNE on FMA MFCC features...\")\n",
    "    \n",
    "    mfcc_flat = mfcc_features.reshape(mfcc_features.shape[0], -1)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=config.RANDOM_SEED)\n",
    "    tsne_features = tsne.fit_transform(mfcc_flat)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(tsne_features[:, 0], tsne_features[:, 1], \n",
    "                          c=labels, cmap='tab10', alpha=0.6, s=50)\n",
    "    plt.colorbar(scatter, label='FMA Genre')\n",
    "    plt.title('t-SNE Visualization of FMA MFCC Features')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    \n",
    "    # Add legend for FMA genres\n",
    "    for i, genre in enumerate(genres):\n",
    "        mask = labels == i\n",
    "        plt.scatter([], [], c=[plt.cm.tab10(i)], label=genre, s=100)\n",
    "    plt.legend(title='FMA Genres', loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8ad3a",
   "metadata": {},
   "source": [
    "## 6. FMA Project Summary\n",
    "\n",
    "Summary statistics for the FMA music genre clustering project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceafeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMA Project summary\n",
    "print(\"=\"*60)\n",
    "print(\"FMA MUSIC GENRE CLUSTERING - PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìÅ FMA Dataset:\")\n",
    "if 'genres' in dir():\n",
    "    print(f\"  Genres: {genres}\")\n",
    "if 'mfcc_features' in dir():\n",
    "    print(f\"  Total samples: {len(labels)}\")\n",
    "    print(f\"  MFCC shape: {mfcc_features.shape}\")\n",
    "if 'mel_features' in dir():\n",
    "    print(f\"  Mel-Spectrogram shape: {mel_features.shape}\")\n",
    "\n",
    "print(\"\\nüß† VAE Models:\")\n",
    "print(f\"  Easy Task (Basic VAE): {'‚úì Trained' if easy_model_path.exists() else '‚úó Not trained'}\")\n",
    "print(f\"  Medium Task (Conv-VAE): Check results directory\")\n",
    "print(f\"  Hard Task (Beta-VAE): {'‚úì Trained' if hard_model_path.exists() else '‚úó Not trained'}\")\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "for task_name, results_dir in results_dirs.items():\n",
    "    if results_dir.exists():\n",
    "        files = list(results_dir.glob('*.png')) + list(results_dir.glob('*.csv'))\n",
    "        print(f\"  {task_name}: {len(files)} output files\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Audio Configuration:\")\n",
    "print(f\"  Sample rate: {config.SAMPLE_RATE} Hz\")\n",
    "print(f\"  Audio duration: {config.AUDIO_DURATION} seconds\")\n",
    "print(f\"  MFCC coefficients: {config.N_MFCC}\")\n",
    "print(f\"  Mel bands: {config.N_MELS}\")\n",
    "\n",
    "print(\"\\nüìå Dataset: Free Music Archive (FMA)\")\n",
    "print(\"   Reference: https://github.com/mdeff/fma\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
